{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPUs if they are available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Model hyperparameters\n",
    "neurons_per_hidden_layer = [20] * 2\n",
    "\n",
    "# Mini-Batch SGD hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmnist_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "    # Computing normalization constants for Fashion-MNIST (commented out since we only need to do this once)\n",
    "    # train_loader, valid_loader = get_fmnist_data_loaders(data_path, 0)\n",
    "    # X, _ = next(iter(train_loader))\n",
    "    # s, m = torch.std_mean(X)\n",
    "\n",
    "\n",
    "    # Data specific transforms\n",
    "    data_mean = (0.2860,)\n",
    "    data_std = (0.3530,)\n",
    "    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = FashionMNIST(root=path, train=False, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example dataset (Fashion MNIST)\n",
    "train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "print(\"Training dataset shape   :\", train_loader.dataset.data.shape)\n",
    "print(\"Validation dataset shape :\", valid_loader.dataset.data.shape)\n",
    "\n",
    "num_to_show = 8\n",
    "images = train_loader.dataset.data[:num_to_show]\n",
    "targets = train_loader.dataset.targets[:num_to_show]\n",
    "labels = [train_loader.dataset.classes[t] for t in targets]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show)\n",
    "\n",
    "for axis, image, label in zip(axes, images, labels):\n",
    "    axis.imshow(image.squeeze(), cmap=\"Greys\")\n",
    "    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_title(f\"{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    # This class will represent a basic neural network layer with a linear function\n",
    "    # and an activation (in this case ReLU)\n",
    "    def __init__(self, in_dimensions, out_dimensions):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dimensions, out_dimensions)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, layer_class=Layer):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            (layer_class(nlminus1, nl) if nlminus1 == nl else Layer(nlminus1, nl))\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        # Set the learning rate\n",
    "        self.lr = lr\n",
    "        # Store the set of parameters that we'll be optimizing\n",
    "        self.parameters = list(parameters)\n",
    "\n",
    "    def step(self):\n",
    "        # Take a step of gradient descent\n",
    "        for ind, parameter in enumerate(self.parameters):\n",
    "            # Compute the update to the parameter\n",
    "            update = self.lr * parameter.grad\n",
    "\n",
    "            # Update the parameter: w <- w - lr * grad\n",
    "            parameter -= update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOptimizer(SGDOptimizer):\n",
    "\n",
    "# Gradient descent with momentum\n",
    "    def __init__(self, parameters, lr=0.01, mu=0.9):\n",
    "        # Set the learning rate\n",
    "        self.lr = lr\n",
    "        # Set the momentum update rate\n",
    "        self.mu = mu\n",
    "        # Store the set of parameters that we'll be optimizing\n",
    "        self.parameters = list(parameters)\n",
    "        # Store the velocity for each parameter\n",
    "        self.velocity = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        # Take a step of gradient descent\n",
    "        for ind, parameter in enumerate(self.parameters):\n",
    "            # First update the velocity\n",
    "            self.velocity[ind] = self.mu * self.velocity[ind] + (1 - self.mu) * parameter.grad\n",
    "\n",
    "            # Then compute the update to the parameter\n",
    "            update = self.lr * self.velocity[ind]\n",
    "\n",
    "            # Finally update the parameter as before\n",
    "            parameter -= update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "class RMSPropOptimizer(SGDOptimizer):\n",
    "    # RMSProp Optimizer\n",
    "    def __init__(self, parameters, lr=0.01, beta=0.9):\n",
    "        # Set the learning rate\n",
    "        self.lr = lr\n",
    "        # Set the scale update rate\n",
    "        self.beta = beta\n",
    "        # Store the set of parameters that we'll be optimizing\n",
    "        self.parameters = list(parameters)\n",
    "        \n",
    "        # Store any values that you'll need for RMSProp here\n",
    "        self.grad = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        # Take a step of gradient descent\n",
    "        for ind, parameter in enumerate(self.parameters):\n",
    "            # First compute any updates needed for RMSProp here\n",
    "            self.grad[ind] = self.beta * self.grad[ind] + (1 - self.beta) * parameter.grad**2\n",
    "\n",
    "            # Then compute the update to the parameter\n",
    "            update = self.lr * (parameter.grad / torch.sqrt(torch.add(self.grad[ind], epsilon)))\n",
    "\n",
    "            # Finally update the parameter as before\n",
    "            parameter -= update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "class AdamOptimizer(SGDOptimizer):\n",
    "    # Adam optimizer\n",
    "    def __init__(self, parameters, lr=0.01, beta1=0.9, beta2=0.99):\n",
    "        # Set the learning rate\n",
    "        self.lr = lr\n",
    "        # Set the scale update rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        \n",
    "        # Store the set of parameters that we'll be optimizing\n",
    "        self.parameters = list(parameters)\n",
    "\n",
    "        # Store any values that you'll need for Adam here\n",
    "        self.t = 0\n",
    "        self.velocity = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.grad = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        # Take a step of gradient descent\n",
    "        for ind, parameter in enumerate(self.parameters):\n",
    "            # First compute any updates needed for Adam here  \n",
    "            self.t += 1      \n",
    "            self.velocity[ind] = self.beta1 * self.velocity[ind] + (1 - self.beta1) * parameter.grad\n",
    "            self.grad[ind] = self.beta2 * self.grad[ind] + (1 - self.beta2) * parameter.grad**2\n",
    "\n",
    "            # Then compute the update to the parameter\n",
    "            numer = (self.velocity[ind] / (1 - self.beta1 ** self.t))\n",
    "            denom = (self.grad[ind] / (1 - self.beta2 ** self.t)) + epsilon\n",
    "            update = self.lr * numer / torch.sqrt(denom)\n",
    "\n",
    "            # Finally update the parameter as before\n",
    "            parameter -= update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(optimizer=SGDOptimizer, \n",
    "              layer_type=Layer, \n",
    "              number_of_hidden_layers=2,\n",
    "              neurons_per_hidden_layer=20, \n",
    "              learning_rate=0.001):\n",
    "    \n",
    "    # Get the dataset\n",
    "    train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "    # The input layer size depends on the dataset\n",
    "    nx = train_loader.dataset.data.shape[1:].numel()\n",
    "\n",
    "    # The output layer size depends on the dataset\n",
    "    ny = len(train_loader.dataset.classes)\n",
    "\n",
    "    # Preprend the input and append the output layer sizes\n",
    "    layer_sizes = [nx] + [neurons_per_hidden_layer] * number_of_hidden_layers + [ny]\n",
    "\n",
    "    # Do model creation here so that the model is recreated each time the cell is run\n",
    "    model = NeuralNetwork(layer_sizes, layer_type).to(device)\n",
    "\n",
    "    t = 0\n",
    "    # Create the optimizer, just like we have with the built-in optimizer\n",
    "    opt = optimizer(model.parameters(), learning_rate)\n",
    "\n",
    "    # A master bar for fancy output progress\n",
    "    mb = master_bar(range(num_epochs))\n",
    "\n",
    "    # Information for plots\n",
    "    mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in mb:\n",
    "\n",
    "        #\n",
    "        # Training\n",
    "        #\n",
    "        model.train()\n",
    "\n",
    "        train_N = len(train_loader.dataset)\n",
    "        num_train_batches = len(train_loader)\n",
    "        train_dataiterator = iter(train_loader)\n",
    "\n",
    "        train_loss_mean = 0\n",
    "\n",
    "        for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "\n",
    "            # Grab the batch of data and send it to the correct device\n",
    "            train_X, train_Y = next(train_dataiterator)\n",
    "            train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "            # Compute the output\n",
    "            train_output = model(train_X)\n",
    "\n",
    "            # Compute loss\n",
    "            train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "            num_in_batch = len(train_X)\n",
    "            tloss = train_loss.item() * num_in_batch / train_N\n",
    "            train_loss_mean += tloss\n",
    "            train_losses.append(train_loss.item())\n",
    "\n",
    "            # Compute gradient\n",
    "            model.zero_grad()\n",
    "            train_loss.backward()\n",
    "            \n",
    "            # Take a step of gradient descent\n",
    "            t += 1\n",
    "            with torch.no_grad():\n",
    "                opt.step()\n",
    "\n",
    "        #\n",
    "        # Validation\n",
    "        #\n",
    "        model.eval()\n",
    "\n",
    "        valid_N = len(valid_loader.dataset)\n",
    "        num_valid_batches = len(valid_loader)\n",
    "\n",
    "        valid_loss_mean = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # valid_loader is probably just one large batch, so not using progress bar\n",
    "            for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "                valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "                valid_output = model(valid_X)\n",
    "\n",
    "                valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "                num_in_batch = len(valid_X)\n",
    "                vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "                valid_loss_mean += vloss\n",
    "                valid_losses.append(valid_loss.item())\n",
    "\n",
    "                # Convert network output into predictions (one-hot -> number)\n",
    "                predictions = valid_output.argmax(1)\n",
    "\n",
    "                # Sum up total number that were correct\n",
    "                valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "        valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "        # Report information\n",
    "        tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "        vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "        vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "        mb.write(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "        # Update plot data\n",
    "        max_loss = max(max(train_losses), max(valid_losses))\n",
    "        min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "        x_margin = 0.2\n",
    "        x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "        y_margin = 0.1\n",
    "        y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "        valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "        valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "        graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "        mb.update_graph(graph_data, x_bounds, y_bounds)\n",
    "\n",
    "    print(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_model(optimizer=SGDOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(optimizer=MomentumOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(optimizer=RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(optimizer=AdamOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):       \n",
    "        mean = x.mean(axis=1, keepdims=True)\n",
    "        var = ((x - mean)**2).mean(axis=1, keepdims=True)\n",
    "        return (x - mean) / torch.sqrt(var + epsilon) \n",
    "    \n",
    "# Replace our Layer class with one that includes layer normalization\n",
    "class LayerNormLayer(nn.Module):\n",
    "    def __init__(self, in_dimensions, out_dimensions):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dimensions, out_dimensions)\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, dimensions, beta=0.9):\n",
    "        super().__init__()\n",
    "        self.beta=beta\n",
    "        self.running_mean = torch.zeros((1, dimensions))\n",
    "        self.running_var = torch.ones((1, dimensions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Needed for GPU compatibility\n",
    "        self.running_mean = self.running_mean.to(x.device)\n",
    "        self.running_var = self.running_var.to(x.device)\n",
    "        \n",
    "        if self.training:\n",
    "            mean = x.mean(axis=0, keepdims=True)\n",
    "            var = ((x - mean)**2).mean(axis=0, keepdims=True)\n",
    "\n",
    "            self.running_mean = self.beta * self.running_mean + (1 - self.beta) * mean\n",
    "            self.running_var = self.beta * self.running_var + (1 - self.beta) * var\n",
    "\n",
    "            return (x - mean) / torch.sqrt(var + epsilon) \n",
    "        else:\n",
    "            return (x - self.running_mean) / torch.sqrt(self.running_var + epsilon) \n",
    "        \n",
    "class BatchNormLayer(nn.Module):\n",
    "    def __init__(self, in_dimensions, out_dimensions):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dimensions, out_dimensions)\n",
    "        self.layer_norm = BatchNorm(out_dimensions)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Layer (for Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, dimensions, *args):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dimensions, dimensions)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(number_of_hidden_layers=20, layer_type=ResidualLayer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
