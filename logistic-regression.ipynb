{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('white')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Test Datasets and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    images, labels, label_names = None, None, list(map(str, range(10)))\n",
    "    if name == 'ones_and_zeros':\n",
    "        mnist = tfds.image_classification.MNIST()\n",
    "        mnist.download_and_prepare()\n",
    "        data = tfds.as_numpy(mnist.as_dataset(split='train', as_supervised=True).batch(5000))\n",
    "        images, labels = next(iter(data))\n",
    "        images, labels = images[labels <= 1][:, :, :, 0].astype(float) / 128 - 1., labels[labels <= 1].astype(float)\n",
    "    if name == 'mnist':\n",
    "        mnist = tfds.image_classification.MNIST()\n",
    "        mnist.download_and_prepare()\n",
    "        data = tfds.as_numpy(mnist.as_dataset(split='train', as_supervised=True).batch(5000))\n",
    "        images, labels = next(iter(data))\n",
    "        images, labels = images[:, :, :, 0].astype(float) / 128 - 1., labels.astype(int)\n",
    "    elif name == 'horses_and_humans':\n",
    "        hh = tfds.image_classification.HorsesOrHumans()\n",
    "        hh.download_and_prepare()\n",
    "        data = tfds.as_numpy(hh.as_dataset(split='train', as_supervised=True).map(lambda x, y: (tf.image.resize(\n",
    "        tf.image.rgb_to_grayscale(x), (48, 48)), y)\n",
    "        ).batch(100000))\n",
    "        images, labels = next(iter(data))\n",
    "        images, labels = images[labels <= 1][:, :, :, 0].astype(float) / 128 - 1., labels[labels <= 1].astype(float)\n",
    "        label_names = ['horse', 'human']\n",
    "    elif name == 'cats_and_dogs':\n",
    "        cd = tfds.image_classification.CatsVsDogs()\n",
    "        cd.download_and_prepare()\n",
    "        data = tfds.as_numpy(cd.as_dataset(split='train', as_supervised=True).map(lambda x, y: (tf.image.resize(\n",
    "        tf.image.rgb_to_grayscale(x), (64, 64)), y)\n",
    "        ).batch(100000))\n",
    "        images, labels = next(iter(data))\n",
    "        images, labels = images[labels <= 1][:, :, :, 0].astype(float) / 128 - 1., labels[labels <= 1].astype(float)\n",
    "        label_names = ['cat', 'dog']\n",
    "    elif name == 'iris':\n",
    "        import sklearn.datasets as datasets\n",
    "        dataset = datasets.load_iris()\n",
    "        features = dataset['data']\n",
    "        targets = dataset['target']\n",
    "        feature_names = dataset['feature_names']\n",
    "        target_names = dataset['target_names']\n",
    "        images, labels = features, targets \n",
    "        label_names = target_names\n",
    "\n",
    "    f, subplots = plt.subplots(8, 8, figsize=(20, 20))\n",
    "    i = 0\n",
    "    for row in subplots:\n",
    "        for subplot in row:\n",
    "            subplot.imshow(images[i], cmap='gray')\n",
    "            subplot.axis('off')\n",
    "            i += 1\n",
    "    plt.show()\n",
    "    return images, labels, label_names\n",
    "\n",
    "def gradient_descent(model, X, y, lr=1e-6, steps=2500, image_shape=None, watch=True):\n",
    "    image_shape = image_shape if model.weights.ndim == 1 else None\n",
    "    dims = np.array(image_shape).prod()\n",
    "\n",
    "    losses = []\n",
    "    progress = tqdm.trange(steps)\n",
    "    if watch:\n",
    "        from IPython import display\n",
    "        f, ax = plt.subplots(X.shape[1] // dims, 3, figsize=(15,8))\n",
    "        ax = np.atleast_2d(ax)\n",
    "\n",
    "    for i in progress:\n",
    "        loss, g = model.nll_and_grad(X, y)\n",
    "        model.weights = model.weights - lr * g\n",
    "        accuracy = model.accuracy(X, y)\n",
    "\n",
    "        losses.append(loss)\n",
    "        progress.set_description('Loss %.2f, accuracy: %.2f' % (loss, accuracy))\n",
    "\n",
    "        # Plotting code\n",
    "        if watch:\n",
    "            [a.cla() for a in ax.flatten()]\n",
    "            [a.axis('off') for a in ax.flatten()[1:]]\n",
    "            display.clear_output(wait =True)\n",
    "            \n",
    "            ax[0, 0].plot(losses)\n",
    "            if not (image_shape is None):\n",
    "                ax[0, 1].imshow(model.weights[:dims].reshape(image_shape))\n",
    "                ax[0, 2].imshow(g[:dims].reshape(image_shape))\n",
    "                ax[0, 1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy))\n",
    "\n",
    "                for j in range(1, ax.shape[0]):\n",
    "                    ax[j, 1].imshow(model.weights[(dims * j):(dims * (j+1))].reshape(image_shape))\n",
    "                    ax[j, 2].imshow(g[(dims * j):(dims * (j+1))].reshape(image_shape))\n",
    "                    ax[j, 0].imshow((X[0, (dims * j):(dims * (j+1))].reshape(image_shape)) )\n",
    "\n",
    "            display.display(f)\n",
    "            time.sleep(0.001)\n",
    "        \n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(X, w):\n",
    "    # Returns a linear function of X (and adds bias)\n",
    "    X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1,))\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (int array): A length N array of predictions in {0, 1}\n",
    "        '''\n",
    "        return (linear_function(X, self.weights) > 0).astype(int)    \n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): A length N vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(linear_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        return np.mean(self.predict(X) == y)    \n",
    "    \n",
    "    def nll(self, X, y):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (int array): A length N vector of labels.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        xw = linear_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -np.sum(np.log(py))    \n",
    "    \n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        xw = linear_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "    \n",
    "    def nll_and_grad(self, X, y):\n",
    "        '''\n",
    "        Compute both the NLL and it's gradient\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing logistic regression and feature transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, label_names = get_dataset('horses_and_humans')\n",
    "\n",
    "image_shape = images[0].shape                # Keep track of the original image shape\n",
    "X = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\n",
    "y = labels\n",
    "print('Image shape: ', images.shape, ', X shape: ', X.shape)\n",
    "\n",
    "model = LogisticRegression(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = gradient_descent(model, X, y, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_probability(X)\n",
    "\n",
    "# Get the probability of the prediction [p(y=1) if prediction is 1 otherwise p(y=0)]\n",
    "probability_of_prediction = np.where(prediction, probabilities, 1 - probabilities)\n",
    "\n",
    "# Show an image and the corresponding prediction\n",
    "plt.imshow(X[0].reshape(image_shape), cmap='gray')\n",
    "print('Prediction: %s, probability: %.3f' % (label_names[prediction[0]], probability_of_prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Train Split\n",
    "def split_data(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
    "\n",
    "    return Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "train_model = LogisticRegression(Xtrain.shape[1])\n",
    "losses = gradient_descent(train_model, Xtrain, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = train_model.accuracy(X = Xtrain, y = ytrain)\n",
    "train_loss = train_model.nll(X = Xtrain, y = ytrain)\n",
    "\n",
    "test_acc = train_model.accuracy(X = Xtest, y = ytest)\n",
    "test_loss = train_model.nll(X = Xtest, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic Feature Transforms\n",
    "Xtrain_quad = [np.append(Xtrain[0], [np.square(Xtrain[0])]).tolist()]\n",
    "Xtest_quad = [np.append(Xtest[0], [np.square(Xtest[0])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtrain)):\n",
    "   Xtrain_quad += [np.append(Xtrain[i], [np.square(Xtrain[i])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtest)):\n",
    "   Xtest_quad += [np.append(Xtest[i], [np.square(Xtest[i])]).tolist()]\n",
    "\n",
    "Xtrain_quad = np.asmatrix(Xtrain_quad)\n",
    "Xtest_quad = np.asmatrix(Xtest_quad)\n",
    "\n",
    "assert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic transforms\n",
    "quad_model = LogisticRegression(Xtrain_quad.shape[1])\n",
    "losses = gradient_descent(quad_model, Xtrain_quad, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = quad_model.accuracy(X = Xtrain_quad, y = ytrain)\n",
    "train_loss = quad_model.nll(X = Xtrain_quad, y = ytrain)\n",
    "\n",
    "test_acc = quad_model.accuracy(X = Xtest_quad, y = ytest)\n",
    "test_loss = quad_model.nll(X = Xtest_quad, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin transforms\n",
    "Xtrain_sin = [np.append(Xtrain[0], [np.sin(10 * Xtrain[0])]).tolist()]\n",
    "Xtest_sin = [np.append(Xtest[0], [np.sin(10 * Xtest[0])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtrain)):\n",
    "   Xtrain_sin += [np.append(Xtrain[i], [np.sin(10 * Xtrain[i])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtest)):\n",
    "   Xtest_sin += [np.append(Xtest[i], [np.sin(10 * Xtest[i])]).tolist()]\n",
    "\n",
    "Xtrain_sin = np.asmatrix(Xtrain_sin)\n",
    "Xtest_sin = np.asmatrix(Xtest_sin)\n",
    "\n",
    "assert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n",
    "\n",
    "# Training model\n",
    "sin_model = LogisticRegression(Xtrain_sin.shape[1])\n",
    "losses = gradient_descent(sin_model, Xtrain_sin, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = sin_model.accuracy(X = Xtrain_sin, y = ytrain)\n",
    "train_loss = sin_model.nll(X = Xtrain_sin, y = ytrain)\n",
    "\n",
    "test_acc = sin_model.accuracy(X = Xtest_sin, y = ytest)\n",
    "test_loss = sin_model.nll(X = Xtest_sin, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, classes, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            classes (int): C, the number of possible outputs\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.classes = classes\n",
    "        self.weights = np.zeros((classes, dims + 1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_predict(self, X):\n",
    "    '''\n",
    "    Predict labels given a set of inputs.\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "    Returns:\n",
    "        pred (int array): A length N array of predictions in {0,...,(C-1)}\n",
    "    '''\n",
    "    W = self.weights\n",
    "    pred_matrix = []\n",
    "    pred = []\n",
    "\n",
    "    for w in W:\n",
    "        pred_matrix += [linear_function(X, w)]\n",
    "\n",
    "    for i in range(0, len(pred_matrix[0])):\n",
    "        pred += [np.argmax(np.squeeze(pred_matrix)[:, i])]\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "MultinomialLogisticRegression.predict = multiclass_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Apply the softmax function to a vector or matrix\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x C matrix of transformed inputs (or a length C vector)\n",
    "    Returns:\n",
    "        probs (array):  An N x C matrix with the softmax function applied to each row\n",
    "    ''' \n",
    "\n",
    "    if x.ndim == 1: return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    smax = []\n",
    "    for row in x:\n",
    "        smax += [np.exp(row) / np.sum(np.exp(row))]\n",
    "\n",
    "    return np.asmatrix(smax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(self, X, y):\n",
    "    '''\n",
    "    Compute the negative log-likelihood loss.\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (int array): A length N vector of labels.\n",
    "    Returns:\n",
    "        nll (float): The NLL loss\n",
    "    '''\n",
    "    X = np.pad(X, ((0, 0), (0, 1)), constant_values = 1.)\n",
    "    W = self.weights\n",
    "    \n",
    "    nll = 0\n",
    "    for i in range(0, len(X)):\n",
    "        term1 = np.dot(np.transpose(X[i]), W[y[i]])\n",
    "\n",
    "        term2 = 0\n",
    "        for j in range(0, len(W)):\n",
    "            term2 += np.exp(np.dot(np.transpose(X[i]), W[j]))\n",
    "\n",
    "        term2 = np.log(term2)\n",
    "        nll += (term1 - term2)\n",
    "\n",
    "    return -1 * nll\n",
    "\n",
    "MultinomialLogisticRegression.nll = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_gradient_c(W, X, y, c):\n",
    "    '''\n",
    "    Compute the negative log-likelihood loss.\n",
    "\n",
    "    Args:\n",
    "        W (array): The C x d weight matrix.\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (int array): A length N vector of labels.\n",
    "        c (int): The class to compute the gradient for\n",
    "    Returns:\n",
    "        grad (array): A length d vector representing the gradient with respect to w_c\n",
    "    '''\n",
    "    grad = 0\n",
    "    X = np.pad(X, ((0, 0), (0, 1)), constant_values = 1.)\n",
    "\n",
    "    for i in range(0, len(X)):\n",
    "        grad += X[i] if y[i] == c else 0\n",
    "        \n",
    "        num = np.dot(X[i].T, np.exp(np.dot(X[i].T, W[c])))\n",
    "        denom = np.sum(np.exp(np.dot(X[i], W.T)), axis = 0)\n",
    "        term2 = num / denom\n",
    "\n",
    "        grad -= term2\n",
    "    \n",
    "    return -1 * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_gradient(self, X, y):\n",
    "    '''\n",
    "    Compute the negative log-likelihood loss.\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (int array): A length N vector of labels.\n",
    "    Returns:\n",
    "        grad (array): A C x d matrix representing the gradient with respect to W\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    W = self.weights\n",
    "    grad = []\n",
    "\n",
    "    for c in range(0, len(W)):\n",
    "        grad += [nll_gradient_c(W, X, y, c)]\n",
    "\n",
    "    return np.array(grad)\n",
    "\n",
    "MultinomialLogisticRegression.nll_gradient = nll_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing multinomial regression and feature transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "images, labels, label_names = get_dataset('mnist')\n",
    "\n",
    "image_shape = images[0].shape                # Keep track of the original image shape\n",
    "X = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\n",
    "y = labels\n",
    "print('Image shape: ', images.shape, ', X shape: ', X.shape)\n",
    "\n",
    "# Create the initial model\n",
    "model = MultinomialLogisticRegression(classes=len(label_names), dims=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X[:500], y[:500]\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X, y)\n",
    "\n",
    "multi_train_model = MultinomialLogisticRegression(classes=len(label_names), dims=Xtrain.shape[1])\n",
    "losses = gradient_descent(multi_train_model, Xtrain, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = multi_train_model.accuracy(X = Xtrain, y = ytrain)\n",
    "train_loss = multi_train_model.nll(X = Xtrain, y = ytrain)\n",
    "\n",
    "test_acc = multi_train_model.accuracy(X = Xtest, y = ytest)\n",
    "test_loss = multi_train_model.nll(X = Xtest, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing learned weights\n",
    "fig, ax = plt.subplots(10)\n",
    "W = multi_train_model.weights\n",
    "\n",
    "for i in range(0, len(W)):\n",
    "    C = W[i][:-1].reshape(28, 28)\n",
    "    ax[i].imshow(C)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic Transforms\n",
    "Xtrain_quad = [np.append(Xtrain[0], [np.square(Xtrain[0])]).tolist()]\n",
    "Xtest_quad = [np.append(Xtest[0], [np.square(Xtest[0])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtrain)):\n",
    "   Xtrain_quad += [np.append(Xtrain[i], [np.square(Xtrain[i])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtest)):\n",
    "   Xtest_quad += [np.append(Xtest[i], [np.square(Xtest[i])]).tolist()]\n",
    "\n",
    "Xtrain_quad = np.asmatrix(Xtrain_quad)\n",
    "Xtest_quad = np.asmatrix(Xtest_quad)\n",
    "\n",
    "assert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n",
    "\n",
    "quad_multi_model = MultinomialLogisticRegression(classes=len(label_names), dims=Xtrain_quad.shape[1])\n",
    "losses = gradient_descent(quad_multi_model, Xtrain_quad, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = quad_multi_model.accuracy(X = Xtrain_quad, y = ytrain)\n",
    "train_loss = quad_multi_model.nll(X = Xtrain_quad, y = ytrain)\n",
    "\n",
    "test_acc = quad_multi_model.accuracy(X = Xtest_quad, y = ytest)\n",
    "test_loss = quad_multi_model.nll(X = Xtest_quad, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin feature transform\n",
    "Xtrain_sin = [np.append(Xtrain[0], [np.sin(10 * Xtrain[0])]).tolist()]\n",
    "Xtest_sin = [np.append(Xtest[0], [np.sin(10 * Xtest[0])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtrain)):\n",
    "   Xtrain_sin += [np.append(Xtrain[i], [np.sin(10 * Xtrain[i])]).tolist()]\n",
    "\n",
    "for i in range(1, len(Xtest)):\n",
    "   Xtest_sin += [np.append(Xtest[i], [np.sin(10 * Xtest[i])]).tolist()]\n",
    "\n",
    "Xtrain_sin = np.asmatrix(Xtrain_sin)\n",
    "Xtest_sin = np.asmatrix(Xtest_sin)\n",
    "\n",
    "assert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n",
    "\n",
    "# Training sin-transformed model\n",
    "sin_multi_model = MultinomialLogisticRegression(classes=len(label_names), dims=Xtrain_sin.shape[1])\n",
    "losses = gradient_descent(sin_multi_model, Xtrain_quad, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n",
    "\n",
    "train_acc = sin_multi_model.accuracy(X = Xtrain_sin, y = ytrain)\n",
    "train_loss = sin_multi_model.nll(X = Xtrain_sin, y = ytrain)\n",
    "\n",
    "test_acc = sin_multi_model.accuracy(X = Xtest_sin, y = ytest)\n",
    "test_loss = sin_multi_model.nll(X = Xtest_sin, y = ytest)\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
