{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports + Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aslist(value):\n",
    "    # Converts iterables to lists and single values to a single-element lists\n",
    "    try:\n",
    "        return list(value)\n",
    "    except:\n",
    "        return (value,)\n",
    "\n",
    "def gradient_descent(model, X, y, lr=1e-6, steps=250):\n",
    "    losses = []\n",
    "    progress = tqdm.trange(steps)\n",
    "    for i in progress:\n",
    "        loss, g = model.nll_and_grad(X, y)\n",
    "        if isinstance(model.weights, np.ndarray):\n",
    "            model.weights = model.weights - lr * g\n",
    "        else:\n",
    "            model.weights = [wi - lr * gi for (wi, gi) in zip(aslist(model.weights), aslist(g))]\n",
    "        accuracy = model.accuracy(X, y)\n",
    "\n",
    "        losses.append(loss)\n",
    "        progress.set_description('Loss %.2f, accuracy: %.2f' % (loss, accuracy))\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def plot_boundary(model, X, y):\n",
    "    xrange = (-X[:, 0].min() + X[:, 0].max()) / 10\n",
    "    yrange = (-X[:, y].min() + X[:, y].max()) / 10\n",
    "    feature_1, feature_2 = np.meshgrid(\n",
    "        np.linspace(X[:, 0].min() - xrange, X[:, 0].max() + xrange),\n",
    "        np.linspace(X[:, 1].min() - yrange, X[:, 1].max() + yrange)\n",
    "    )\n",
    "    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "    y_pred = np.reshape(model.predict(grid), feature_1.shape)\n",
    "    display = DecisionBoundaryDisplay(\n",
    "        xx0=feature_1, xx1=feature_2, response=y_pred\n",
    "    )\n",
    "    display.plot()\n",
    "    display.ax_.scatter(\n",
    "        X[:, 0], X[:, 1], c=y.flatten(), edgecolor=\"black\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradValue:\n",
    "    '''\n",
    "    Base class for automatic differentiation operations. Represents variable delcaration.\n",
    "    Subclasses will overwrite func and grads to define new operations.\n",
    "\n",
    "    Properties:\n",
    "        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n",
    "        args    (list): A list of raw values of each input (as floats)\n",
    "        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n",
    "        value   (float): The value of the result of this operation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.parents = list(args)\n",
    "        self.args = [arg.value if isinstance(arg, AutogradValue) else arg for arg in self.parents]\n",
    "        self.grad = 0.\n",
    "        self.value = self.forward_pass()\n",
    "\n",
    "    def func(self, input):\n",
    "        '''\n",
    "        Compute the value of the operation given the inputs.\n",
    "        For declaring a variable, this is just the identity function (return the input).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            value (float): The result of the operation\n",
    "        '''\n",
    "        return input\n",
    "\n",
    "    def grads(self, *args):\n",
    "        '''\n",
    "        Compute the derivative of the operation with respect to each input.\n",
    "        In the base case the derivative of the identity function is just 1. (da/da = 1).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            grads (tuple): The derivative of the operation with respect to each input\n",
    "                            Here there is only a single input, so we return a length-1 tuple.\n",
    "        '''\n",
    "        return (1,)\n",
    "    \n",
    "    def forward_pass(self):\n",
    "        # Calls func to compute the value of this operation \n",
    "        return self.func(*self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Python magic function for string representation.\n",
    "        return str(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "    \n",
    "class _neg(AutogradValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        return a - b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., -1.\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return (b, a)\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        return a / b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return (1. / b, -a / b**2.)\n",
    "    \n",
    "class _exp(AutogradValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        return np.exp(a)\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (np.exp(a),)\n",
    "    \n",
    "class _log(AutogradValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        return np.log(a)\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (1. / a, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_print(a):\n",
    "    if isinstance(a, AutogradValue): \n",
    "        print(a.value)\n",
    "        [graph_print(p) for p in a.parents]\n",
    "    else:\n",
    "        print(a)\n",
    "\n",
    "a = AutogradValue(5.)\n",
    "b = AutogradValue(2.)\n",
    "c = log((a + 5) * b)\n",
    "graph_print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass function\n",
    "def backward_pass(self):\n",
    "\n",
    "    grads = self.grads(*self.args)\n",
    "    \n",
    "    for p in self.parents:\n",
    "        if isinstance(p, AutogradValue):\n",
    "            p.grad += self.grad * grads[self.parents.index(p)]\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass\n",
    "\n",
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "\n",
    "    visited = [self]\n",
    "    queue = [self]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop()\n",
    "\n",
    "        for p in node.parents:\n",
    "            if isinstance(p, AutogradValue):\n",
    "                try:\n",
    "                    visited.remove(p)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                visited.append(p)\n",
    "                queue.append(p)\n",
    "\n",
    "    for node in visited:\n",
    "        node.backward_pass()\n",
    "\n",
    "AutogradValue.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backward\n",
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "L.backward()\n",
    "print(a.grad, b.grad) # -0.06666666666666667 -0.3333333333333333\n",
    "\n",
    "a = np.array([AutogradValue(5), AutogradValue(2)])\n",
    "L = np.dot(a, a)\n",
    "L.backward()\n",
    "print(a[0].grad, a[1].grad) # 10.0 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_array(a):\n",
    "    '''\n",
    "    Wraps the elements of an array with AutogradValue\n",
    "\n",
    "    Args:\n",
    "        a (array of float): The array to wrap\n",
    "    Returns:\n",
    "        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n",
    "    '''\n",
    "    g = []\n",
    "    for i in a:\n",
    "        g += [[AutogradValue(j) for j in i]]\n",
    "\n",
    "    return np.array(g)\n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    '''\n",
    "    Unwraps the gradient of an array with AutogradValues\n",
    "\n",
    "    Args:\n",
    "        a (array of AutogradValue): The array to unwrap\n",
    "    Returns:\n",
    "        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n",
    "    '''\n",
    "    g = []\n",
    "    for i in a:\n",
    "        g += [[j.grad for j in i]]\n",
    "\n",
    "    return np.array(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1, 1))\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (array): A (d+1) x 1 vector of weights.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of f(X).\n",
    "        '''\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): An N x 1 column vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): An N x 1 column vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(np.log(py)).sum()\n",
    "    \n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:  \n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "    \n",
    "    def nll_and_grad_no_autodiff(self, X, y):\n",
    "        # Compute nll_and_grad without automatic diferentiation\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)\n",
    "\n",
    "    def nll_and_grad(self, X, y):\n",
    "        W = wrap_array(self.weights)\n",
    "        L = LogisticRegression.nll(self, X, y, W)\n",
    "        L.backward()\n",
    "\n",
    "        grad = unwrap_gradient(W)\n",
    "\n",
    "        return L.value, grad  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(LogisticRegression):\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        ## YOUR CODE HERE\n",
    "        self.weights = [np.random.normal(scale=1., size=(hidden_sizes[0], dims))]\n",
    "\n",
    "        for i in range(0, len(hidden_sizes) - 1):\n",
    "            shape = (hidden_sizes[i], hidden_sizes[i + 1])\n",
    "            self.weights += [np.random.normal(scale=1., size=shape)]\n",
    "\n",
    "        self.weights += [(np.random.normal(scale=1., size=(hidden_sizes[-1], 1)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(self, X, w):\n",
    "    '''\n",
    "    Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        w (list of arrays): A list of weight matrices\n",
    "    Returns:\n",
    "        pred (array): An N x 1 matrix of f(X).\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    neurons = sigmoid(np.dot(X, np.array(w[0]).T))\n",
    "    \n",
    "    for i in range(1, len(w) - 1):\n",
    "        neurons = sigmoid(np.dot(neurons, np.array(w[i]).T))\n",
    "\n",
    "    pred = np.dot(neurons, w[-1])\n",
    "    return pred.reshape(-1, 1)\n",
    "\n",
    "NeuralNetwork.prediction_function = prediction_function\n",
    "\n",
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    grad = []\n",
    "    W = [wrap_array(w) for w in self.weights]\n",
    "    \n",
    "    L = NeuralNetwork.nll(self, X, y, W)\n",
    "    L.backward()\n",
    "\n",
    "    grad = [unwrap_gradient(w) for w in W]\n",
    "\n",
    "    return L.value, grad  \n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network!\n",
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardValue(AutogradValue):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.forward_grads = {self: 1}\n",
    "\n",
    "# Define our inputs as ForwardValue objects\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "# Perform operations\n",
    "c = a * b\n",
    "g = 3 * c + a\n",
    "\n",
    "c.forward_grads = {a: 2, b: 5}  # dc/da and dc/db\n",
    "g.forward_grads = {a: 3 * 2 + 1, b: 3 * 5} # dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db\n",
    "\n",
    "def forward_pass(self):\n",
    "    # Clear forward_grads if it exists\n",
    "    self.forward_grads = {} \n",
    "    \n",
    "    grads = self.grads(*self.args)\n",
    "\n",
    "    for i in range(len(self.parents)):\n",
    "        if isinstance(self.parents[i], AutogradValue):\n",
    "            for key, value in self.parents[i].forward_grads.items():\n",
    "                update = value * grads[i]\n",
    "                if key in self.forward_grads:\n",
    "                    self.forward_grads[key] += update\n",
    "                else:\n",
    "                    self.forward_grads[key] = update\n",
    "\n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.args)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "AutogradValue.forward_pass = forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "L = -log(5 *b + a)\n",
    "\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "print('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db)) # dL/da = -0.067, dL/db = -0.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModeNeuralNetwork(NeuralNetwork):\n",
    "\n",
    "    def wrap_array_f(self, a):\n",
    "        '''\n",
    "        Wraps the elements of an array with AutogradValue\n",
    "\n",
    "        Args:\n",
    "            a (array of float): The array to wrap\n",
    "        Returns:\n",
    "            g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n",
    "        '''\n",
    "        g = []\n",
    "        for i in a:\n",
    "            g += [[ForwardValue(j) for j in i]]\n",
    "\n",
    "        return np.array(g)\n",
    "\n",
    "    def unwrap_gradient_f(self, L, a):\n",
    "        '''\n",
    "        Unwraps the gradient of an array with AutogradValues\n",
    "\n",
    "        Args:\n",
    "            a (array of AutogradValue): The array to unwrap\n",
    "        Returns:\n",
    "            g (array of float): An array g, such that g[i,j] = a[i,j].grad\n",
    "        '''\n",
    "        g = []\n",
    "        for i in a:\n",
    "            g += [[L.forward_grads[j] for j in i]]\n",
    "\n",
    "        return np.array(g)\n",
    "    \n",
    "\n",
    "    def nll_and_grad(self, X, y):\n",
    "\n",
    "        W = [self.wrap_array_f(w) for w in self.weights]\n",
    "\n",
    "        L = self.nll(X, y, W)\n",
    "        L.forward_pass()\n",
    "\n",
    "        grad = [self.unwrap_gradient_f(L, w) for w in W]\n",
    "\n",
    "        return L.value, grad  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward\n",
    "\n",
    "X, y = make_moons(100, noise=0.1)\n",
    "model = ForwardModeNeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
